"""
AI Chain Service with LangChain + Gemini Integration

This module provides LangChain-based conversation management with Google Gemini.
It handles conversation memory, system prompts, and AI response generation.
"""

import os
import json
import logging
from typing import Dict, Any, Optional
from datetime import datetime

# LangChain imports
from langchain.memory import ConversationBufferWindowMemory
from langchain.schema import BaseMessage, HumanMessage, AIMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser

# Configure logging
logger = logging.getLogger(__name__)

# Global conversation memories (session-based)
conversation_memories: Dict[str, ConversationBufferWindowMemory] = {}

# AI configuration
AI_CONFIG_FILE = "app/ai_schema.json"
DEFAULT_MODEL = "gemini-1.5-flash"
DEFAULT_TEMPERATURE = 0.7
DEFAULT_MAX_TOKENS = 1000
MEMORY_WINDOW = 10


def load_ai_config() -> Dict[str, Any]:
    """
    Load AI configuration from JSON file.
    """
    try:
        if os.path.exists(AI_CONFIG_FILE):
            with open(AI_CONFIG_FILE, 'r', encoding='utf-8') as f:
                config = json.load(f)
                logger.info("‚úÖ AI configuration loaded from file")
                return config
        else:
            logger.warning("‚ö†Ô∏è AI config file not found, using defaults")
            return get_default_ai_config()
    except Exception as e:
        logger.error(f"‚ùå Error loading AI config: {str(e)}")
        return get_default_ai_config()


def get_default_ai_config() -> Dict[str, Any]:
    """
    Get default AI configuration.
    """
    return {
        "system_prompt": """Voc√™ √© um assistente jur√≠dico especializado de um escrit√≥rio de advocacia no Brasil.

DIRETRIZES IMPORTANTES:
- Responda SEMPRE em portugu√™s brasileiro
- Mantenha respostas profissionais, concisas e humanizadas
- N√ÉO forne√ßa aconselhamento jur√≠dico espec√≠fico (respeitar OAB)
- Demonstre empatia e urg√™ncia quando apropriado
- Use linguagem natural, curta e clara (n√£o pare√ßa rob√≥tico)
- Reforce a autoridade do escrit√≥rio
- Crie valor percebido antes da transfer√™ncia
- Capture gatilhos de necessidade (prazos, audi√™ncias, urg√™ncia)

CONTEXTO ESPECIAL:
- Voc√™ conduz o lead at√© deix√°-lo pronto para o advogado
- Use as informa√ß√µes coletadas para personalizar respostas
- Adapte o tom baseado na plataforma ([Platform: WEB] ou [Platform: WHATSAPP])
- Foque em qualificar e aquecer o lead

√ÅREAS DE ESPECIALIZA√á√ÉO:
- Direito Penal
- Direito Trabalhista  
- Direito Civil
- Direito de Fam√≠lia
- Sa√∫de/Liminares
- Outras √°reas

FORMATO DE RESPOSTA:
- Para WhatsApp: M√°ximo 2 par√°grafos, linguagem mais direta
- Para Web: M√°ximo 3 par√°grafos, pode ser mais detalhado
- Sempre termine preparando para a transfer√™ncia quando apropriado
- Use emojis moderadamente no WhatsApp

TAREFAS ESPECIAIS:
- Transmitir autoridade e confian√ßa desde o in√≠cio
- Coletar dados essenciais de forma natural
- Qualificar a √°rea jur√≠dica corretamente
- Capturar urg√™ncia e contexto do caso
- Preparar o lead para transfer√™ncia humanizada

Voc√™ tem acesso ao hist√≥rico da conversa para fornecer respostas contextualizadas.""",
        "ai_config": {
            "model": DEFAULT_MODEL,
            "temperature": DEFAULT_TEMPERATURE,
            "max_tokens": DEFAULT_MAX_TOKENS,
            "memory_window": MEMORY_WINDOW,
            "timeout": 30
        }
    }


def get_conversation_memory(session_id: str) -> ConversationBufferWindowMemory:
    """
    Get or create conversation memory for a session.
    """
    if session_id not in conversation_memories:
        conversation_memories[session_id] = ConversationBufferWindowMemory(
            k=MEMORY_WINDOW,
            return_messages=True,
            memory_key="chat_history"
        )
        logger.info(f"üß† Created new conversation memory for session: {session_id}")
    
    return conversation_memories[session_id]


def clear_conversation_memory(session_id: str) -> bool:
    """
    Clear conversation memory for a specific session.
    """
    try:
        if session_id in conversation_memories:
            conversation_memories[session_id].clear()
            logger.info(f"üßπ Cleared conversation memory for session: {session_id}")
            return True
        else:
            logger.warning(f"‚ö†Ô∏è No memory found for session: {session_id}")
            return False
    except Exception as e:
        logger.error(f"‚ùå Error clearing memory for session {session_id}: {str(e)}")
        return False


def get_conversation_summary(session_id: str) -> str:
    """
    Get a summary of the conversation for a session.
    """
    try:
        if session_id in conversation_memories:
            memory = conversation_memories[session_id]
            messages = memory.chat_memory.messages
            
            if not messages:
                return "No conversation history"
            
            # Create a simple summary
            summary_parts = []
            for message in messages[-6:]:  # Last 6 messages
                if isinstance(message, HumanMessage):
                    summary_parts.append(f"User: {message.content[:100]}...")
                elif isinstance(message, AIMessage):
                    summary_parts.append(f"AI: {message.content[:100]}...")
            
            return "\n".join(summary_parts)
        else:
            return "No conversation found"
    except Exception as e:
        logger.error(f"‚ùå Error getting conversation summary: {str(e)}")
        return "Error retrieving summary"


class AIOrchestrator:
    """
    Main AI orchestrator using LangChain + Gemini.
    """
    
    def __init__(self):
        self.config = load_ai_config()
        self.ai_config = self.config.get("ai_config", {})
        self.system_prompt = self.config.get("system_prompt", "")
        self.llm = None
        self.chain = None
        self._initialize_llm()
    
    def _initialize_llm(self):
        """
        Initialize the LangChain LLM with Gemini.
        """
        try:
            api_key = os.getenv("GEMINI_API_KEY")
            if not api_key:
                logger.error("‚ùå GEMINI_API_KEY not found in environment variables")
                return
            
            # Initialize Gemini LLM
            self.llm = ChatGoogleGenerativeAI(
                model=self.ai_config.get("model", DEFAULT_MODEL),
                temperature=self.ai_config.get("temperature", DEFAULT_TEMPERATURE),
                max_tokens=self.ai_config.get("max_tokens", DEFAULT_MAX_TOKENS),
                google_api_key=api_key
            )
            
            # Create the conversation chain
            self._create_chain()
            
            logger.info(f"‚úÖ AI Orchestrator initialized with model: {self.ai_config.get('model', DEFAULT_MODEL)}")
            
        except Exception as e:
            logger.error(f"‚ùå Error initializing AI Orchestrator: {str(e)}")
            self.llm = None
            self.chain = None
    
    def _create_chain(self):
        """
        Create the LangChain conversation chain.
        """
        try:
            if not self.llm:
                logger.error("‚ùå Cannot create chain: LLM not initialized")
                return
            
            # Create the prompt template
            prompt = ChatPromptTemplate.from_messages([
                ("system", self.system_prompt),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{input}")
            ])
            
            # Create the chain
            self.chain = (
                RunnablePassthrough.assign(
                    chat_history=lambda x: x["chat_history"]
                )
                | prompt
                | self.llm
                | StrOutputParser()
            )
            
            logger.info("‚úÖ LangChain conversation chain created")
            
        except Exception as e:
            logger.error(f"‚ùå Error creating conversation chain: {str(e)}")
            self.chain = None
    
    async def generate_response(
        self, 
        message: str, 
        session_id: str = "default",
        context: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Generate AI response using LangChain + Gemini.
        """
        try:
            if not self.chain:
                logger.error("‚ùå AI chain not available")
                return "Desculpe, o servi√ßo de IA n√£o est√° dispon√≠vel no momento."
            
            # Get conversation memory
            memory = get_conversation_memory(session_id)
            
            # Add platform context to message if provided
            enhanced_message = message
            if context and context.get("platform"):
                platform = context["platform"].upper()
                enhanced_message = f"[Platform: {platform}] {message}"
            
            # Prepare input for the chain
            chain_input = {
                "input": enhanced_message,
                "chat_history": memory.chat_memory.messages
            }
            
            logger.info(f"ü§ñ Generating AI response for session: {session_id}")
            
            # Generate response
            response = await self.chain.ainvoke(chain_input)
            
            # Save to memory
            memory.save_context(
                {"input": enhanced_message},
                {"output": response}
            )
            
            logger.info(f"‚úÖ AI response generated for session: {session_id}")
            return response
            
        except Exception as e:
            logger.error(f"‚ùå Error generating AI response: {str(e)}")
            return "Desculpe, ocorreu um erro ao processar sua mensagem. Como posso ajud√°-lo?"
    
    def is_available(self) -> bool:
        """
        Check if AI service is available.
        """
        return self.llm is not None and self.chain is not None


# Global AI orchestrator instance
ai_orchestrator = AIOrchestrator()


# Main service functions
async def process_chat_message(
    message: str, 
    session_id: str = "default",
    context: Optional[Dict[str, Any]] = None
) -> str:
    """
    Process chat message using LangChain + Gemini.
    """
    try:
        logger.info(f"üì® Processing chat message for session: {session_id}")
        
        if not ai_orchestrator.is_available():
            logger.error("‚ùå AI orchestrator not available")
            return "Desculpe, o servi√ßo de IA n√£o est√° dispon√≠vel no momento. Como posso ajud√°-lo?"
        
        response = await ai_orchestrator.generate_response(message, session_id, context)
        return response
        
    except Exception as e:
        logger.error(f"‚ùå Error in process_chat_message: {str(e)}")
        return "Desculpe, ocorreu um erro ao processar sua mensagem. Tente novamente."


async def get_ai_service_status() -> Dict[str, Any]:
    """
    Get AI service status.
    """
    try:
        api_key_configured = bool(os.getenv("GEMINI_API_KEY"))
        ai_available = ai_orchestrator.is_available()
        
        return {
            "service": "ai_chain_langchain_gemini",
            "status": "active" if ai_available else "configuration_required",
            "ai_available": ai_available,
            "api_key_configured": api_key_configured,
            "model": ai_orchestrator.ai_config.get("model", DEFAULT_MODEL),
            "memory_sessions": len(conversation_memories),
            "features": [
                "langchain_integration",
                "conversation_memory",
                "google_gemini_api",
                "session_management",
                "context_awareness",
                "platform_adaptation"
            ],
            "configuration_notes": [
                "Set GEMINI_API_KEY environment variable",
                "Configure ai_schema.json for custom prompts"
            ] if not api_key_configured else []
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error getting AI service status: {str(e)}")
        return {
            "service": "ai_chain_langchain_gemini",
            "status": "error",
            "error": str(e),
            "ai_available": False
        }


# Initialize on import
logger.info("üöÄ AI Chain service loaded with LangChain + Gemini integration")